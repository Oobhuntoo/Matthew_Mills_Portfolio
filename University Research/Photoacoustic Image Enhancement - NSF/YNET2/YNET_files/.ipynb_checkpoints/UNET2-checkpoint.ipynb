{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, BatchNormalization, Activation\n",
    "from keras.engine.input_layer import Input\n",
    "import keras\n",
    "import time\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inline\n",
    "# y_predict = (y_1,….,y_)\n",
    "# —> (y_1,…,y_n)/sum(y_i)\n",
    "# y_true = (z_1,…,z_n)\n",
    "# —-> (z_1,…,z_n)/sum(z_i)\n",
    "\n",
    "# KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_custom_loss_function(y_actual, y_predicted):\n",
    "    #y_actual = y_actual / np.sum(y_actual) * 128\n",
    "    #y_predicted = y_predicted / np.sum(y_predicted) * 128\n",
    "    #print(y_actual)\n",
    "    custom_loss_value = keras.losses.mean_squared_error(y_actual, y_predicted) # / (128 * 128)\n",
    "    #custom_loss_value = keras.losses.kullback_leibler_divergence(y_actual, y_predicted)\n",
    "    return custom_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_set(path):\n",
    "    \n",
    "    data = []\n",
    "    files = list(glob.glob(path))\n",
    "    files.sort()\n",
    "    \n",
    "    print(\"Files\")\n",
    "    \n",
    "    for myFile in files:\n",
    "        print(myFile)\n",
    "        image = cv2.imread(myFile)#.astype('float32')\n",
    "#         plt.figure()\n",
    "#         plt.imshow(image)\n",
    "        image = cv2.resize(image, (128,128))\n",
    "        #print(image)\n",
    "\n",
    "        data.append(image[:,:,0])\n",
    "        \n",
    "    np_data = np.array(data)\n",
    "\n",
    "    print()\n",
    "    \n",
    "    return np_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_layer, start_neurons, dropout = 0.05):\n",
    "    \n",
    "    #print(input_layer.shape)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(input_layer)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    \n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(dropout)(pool1)\n",
    "    \n",
    "\n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(dropout)(pool2)\n",
    "\n",
    "\n",
    "\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(dropout)(pool3)\n",
    "\n",
    "\n",
    "\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(dropout)(pool4)\n",
    "\n",
    "\n",
    "    # Middle\n",
    "\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(pool4)\n",
    "    convm = BatchNormalization()(convm)\n",
    "    convm = Activation('relu')(convm)\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(convm)\n",
    "    convm = BatchNormalization()(convm)\n",
    "    convm = Activation('relu')(convm)\n",
    "    \n",
    "\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer = 'he_normal')(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(0.5)(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(uconv4)\n",
    "    uconv4 = BatchNormalization()(uconv4)\n",
    "    uconv4 = Activation('relu')(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(uconv4)\n",
    "    uconv4 = BatchNormalization()(uconv4)\n",
    "    uconv4 = Activation('relu')(uconv4)\n",
    "\n",
    "\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer = 'he_normal')(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    uconv3 = Dropout(dropout)(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(uconv3)\n",
    "    uconv3 = BatchNormalization()(uconv3)  \n",
    "    uconv3 = Activation('relu')(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(uconv3)\n",
    "    uconv3 = BatchNormalization()(uconv3)\n",
    "    uconv3 = Activation('relu')(uconv3)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer = 'he_normal')(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(dropout)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "    uconv2 = Activation('relu')(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "    uconv2 = Activation('relu')(uconv2)\n",
    "    \n",
    "    \n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer = 'he_normal')(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(dropout)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)  \n",
    "    uconv1 = Activation('relu')(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), padding=\"same\", kernel_initializer = 'he_normal')(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    uconv1 = Activation('relu')(uconv1)\n",
    "    \n",
    "    \n",
    "    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\", kernel_initializer = 'he_normal')(uconv1)\n",
    "    output_layer = Conv2D(1, (1, 1))(uconv1)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "# def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "#     \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "#     # first layer\n",
    "#     x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "#               kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "#     if batchnorm:\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     # second layer\n",
    "#     x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "#               kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "#     if batchnorm:\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     return x\n",
    "\n",
    "# def build_model(input_img, n_filters = 16, dropout = 0.05, batchnorm = True):\n",
    "#     \"\"\"Function to define the UNET Model\"\"\"\n",
    "#     # Contracting Path\n",
    "#     c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "#     p1 = MaxPooling2D((2, 2))(c1)\n",
    "#     p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "#     c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "#     p2 = MaxPooling2D((2, 2))(c2)\n",
    "#     p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "#     c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "#     p3 = MaxPooling2D((2, 2))(c3)\n",
    "#     p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "#     c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "#     p4 = MaxPooling2D((2, 2))(c4)\n",
    "#     p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "#     c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "#     # Expansive Path\n",
    "#     u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "#     u6 = concatenate([u6, c4])\n",
    "#     u6 = Dropout(dropout)(u6)\n",
    "#     c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "#     u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "#     u7 = concatenate([u7, c3])\n",
    "#     u7 = Dropout(dropout)(u7)\n",
    "#     c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "#     u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "#     u8 = concatenate([u8, c2])\n",
    "#     u8 = Dropout(dropout)(u8)\n",
    "#     c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "#     u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "#     u9 = concatenate([u9, c1])\n",
    "#     u9 = Dropout(dropout)(u9)\n",
    "#     c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "# #     outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "#     outputs = Conv2D(1, (1, 1))(c9)\n",
    "    \n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files\n",
      "training/images/time_reversal_data\\recon01.png\n",
      "training/images/time_reversal_data\\recon02.png\n",
      "training/images/time_reversal_data\\recon03.png\n",
      "training/images/time_reversal_data\\recon04.png\n",
      "training/images/time_reversal_data\\recon05.png\n",
      "training/images/time_reversal_data\\recon06.png\n",
      "training/images/time_reversal_data\\recon07.png\n",
      "training/images/time_reversal_data\\recon08.png\n",
      "training/images/time_reversal_data\\recon09.png\n",
      "training/images/time_reversal_data\\recon10.png\n",
      "training/images/time_reversal_data\\recon11.png\n",
      "training/images/time_reversal_data\\recon12.png\n",
      "training/images/time_reversal_data\\recon13.png\n",
      "training/images/time_reversal_data\\recon14.png\n",
      "training/images/time_reversal_data\\recon15.png\n",
      "training/images/time_reversal_data\\recon16.png\n",
      "training/images/time_reversal_data\\recon17.png\n",
      "training/images/time_reversal_data\\recon18.png\n",
      "training/images/time_reversal_data\\recon19.png\n",
      "training/images/time_reversal_data\\recon20.png\n",
      "training/images/time_reversal_data\\recon21.png\n",
      "training/images/time_reversal_data\\recon22.png\n",
      "training/images/time_reversal_data\\recon23.png\n",
      "training/images/time_reversal_data\\recon24.png\n",
      "training/images/time_reversal_data\\recon25.png\n",
      "training/images/time_reversal_data\\recon26.png\n",
      "training/images/time_reversal_data\\recon27.png\n",
      "training/images/time_reversal_data\\recon28.png\n",
      "\n",
      "Files\n",
      "test/images/time_reversal_data\\recon01.png\n",
      "test/images/time_reversal_data\\recon02.png\n",
      "test/images/time_reversal_data\\recon03.png\n",
      "test/images/time_reversal_data\\recon04.png\n",
      "test/images/time_reversal_data\\recon05.png\n",
      "test/images/time_reversal_data\\recon06.png\n",
      "test/images/time_reversal_data\\recon07.png\n",
      "test/images/time_reversal_data\\recon08.png\n",
      "test/images/time_reversal_data\\recon09.png\n",
      "test/images/time_reversal_data\\recon10.png\n",
      "test/images/time_reversal_data\\recon11.png\n",
      "test/images/time_reversal_data\\recon12.png\n",
      "\n",
      "Files\n",
      "training/images\\21_training.tif\n",
      "training/images\\22_training.tif\n",
      "training/images\\23_training.tif\n",
      "training/images\\24_training.tif\n",
      "training/images\\25_training.tif\n",
      "training/images\\26_training.tif\n",
      "training/images\\27_training.tif\n",
      "training/images\\28_training.tif\n",
      "training/images\\29_training.tif\n",
      "training/images\\30_training.tif\n",
      "training/images\\31_training.tif\n",
      "training/images\\32_training.tif\n",
      "training/images\\33_training.tif\n",
      "training/images\\34_training.tif\n",
      "training/images\\35_training.tif\n",
      "training/images\\36_training.tif\n",
      "training/images\\37_training.tif\n",
      "training/images\\38_training.tif\n",
      "training/images\\39_training.tif\n",
      "training/images\\40_training.tif\n",
      "training/images\\41_training.tif\n",
      "training/images\\42_training.tif\n",
      "training/images\\43_training.tif\n",
      "training/images\\44_training.tif\n",
      "training/images\\45_training.tif\n",
      "training/images\\46_training.tif\n",
      "training/images\\47_training.tif\n",
      "training/images\\48_training.tif\n",
      "\n",
      "Files\n",
      "test/images\\01_test.tif\n",
      "test/images\\02_test.tif\n",
      "test/images\\03_test.tif\n",
      "test/images\\04_test.tif\n",
      "test/images\\05_test.tif\n",
      "test/images\\06_test.tif\n",
      "test/images\\07_test.tif\n",
      "test/images\\08_test.tif\n",
      "test/images\\09_test.tif\n",
      "test/images\\10_test.tif\n",
      "test/images\\11_test.tif\n",
      "test/images\\12_test.tif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(128, 128, 1))\n",
    "output_layer = build_model(input_layer, 16)\n",
    "\n",
    "\n",
    "x_train_path = r\"training/images/time_reversal_data/*.png\"\n",
    "x_test_path = r\"test/images/time_reversal_data/*.png\"\n",
    "y_train_path = r\"training/images/*.tif\"\n",
    "y_test_path = r\"test/images/*.tif\"\n",
    "\n",
    "x_train = make_image_set(x_train_path)\n",
    "x_test = make_image_set(x_test_path)\n",
    "y_train = make_image_set(y_train_path)\n",
    "y_test = make_image_set(y_test_path)\n",
    "\n",
    "x_train = np.reshape(x_train, (28,128,128,1))\n",
    "x_test = np.reshape(x_test, (12,128,128,1))\n",
    "\n",
    "y_train = np.reshape(y_train, (28,128,128,1))\n",
    "y_test = np.reshape(y_test, (12,128,128,1))\n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    return (x.astype(np.float32) - 128.0) / 128.0\n",
    "\n",
    "x_train = norm(x_train)\n",
    "x_test = norm(x_test)\n",
    "y_train = norm(y_train)\n",
    "y_test = norm(y_test)\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "#     fill_mode=\"wrap\",\n",
    "#     rescale=4)\n",
    "\n",
    "# imageGen = datagen.flow(x_train, y_train, batch_size=8, shuffle=True)\n",
    "# #\n",
    "# datagen.fit(x_train)\n",
    "\n",
    "x_train_gen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "y_train_gen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "x_train_gen.fit(x_train, seed=1)\n",
    "y_train_gen.fit(y_train, seed=1)\n",
    "train_gen = x_train_gen.flow(x_train, batch_size=8, seed=1)\n",
    "mask_gen = y_train_gen.flow(y_train, batch_size=8, seed=1)\n",
    "\n",
    "imageGen = zip(train_gen, mask_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 128, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 128, 128, 16) 160         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 128, 128, 16) 64          conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 128, 128, 16) 0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 128, 128, 16) 2320        activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 128, 128, 16) 64          conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 128, 128, 16) 0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling2D) (None, 64, 64, 16)   0           activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 64, 64, 16)   0           max_pooling2d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 64, 64, 32)   4640        dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 64, 64, 32)   128         conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 64, 64, 32)   0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 64, 64, 32)   9248        activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 64, 64, 32)   128         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 64, 64, 32)   0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling2D) (None, 32, 32, 32)   0           activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 32, 32, 32)   0           max_pooling2d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 32, 32, 64)   18496       dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 32, 32, 64)   256         conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 32, 32, 64)   0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 32, 32, 64)   36928       activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 32, 32, 64)   256         conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 32, 32, 64)   0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling2D) (None, 16, 16, 64)   0           activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 16, 16, 64)   0           max_pooling2d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 16, 16, 128)  73856       dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 16, 16, 128)  512         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 16, 16, 128)  0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 16, 16, 128)  147584      activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 16, 16, 128)  512         conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 16, 16, 128)  0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling2D) (None, 8, 8, 128)    0           activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 8, 8, 128)    0           max_pooling2d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 8, 8, 256)    295168      dropout_100[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 8, 8, 256)    1024        conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 8, 8, 256)    0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 8, 8, 256)    590080      activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 8, 8, 256)    1024        conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 8, 8, 256)    0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_49 (Conv2DTran (None, 16, 16, 128)  295040      activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 16, 16, 256)  0           conv2d_transpose_49[0][0]        \n",
      "                                                                 activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 16, 16, 256)  0           concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 16, 16, 128)  295040      dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 16, 16, 128)  512         conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 16, 16, 128)  0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 16, 16, 128)  147584      activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 16, 16, 128)  512         conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 16, 16, 128)  0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_50 (Conv2DTran (None, 32, 32, 64)   73792       activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 32, 32, 128)  0           conv2d_transpose_50[0][0]        \n",
      "                                                                 activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 32, 32, 128)  0           concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 32, 32, 64)   73792       dropout_102[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 32, 32, 64)   256         conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 32, 32, 64)   0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, 32, 32, 64)   36928       activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 32, 32, 64)   256         conv2d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 32, 32, 64)   0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_51 (Conv2DTran (None, 64, 64, 32)   18464       activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 64, 64, 64)   0           conv2d_transpose_51[0][0]        \n",
      "                                                                 activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 64, 64, 64)   0           concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)             (None, 64, 64, 32)   18464       dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 64, 64, 32)   128         conv2d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 64, 64, 32)   0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, 64, 64, 32)   9248        activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 64, 64, 32)   128         conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 64, 64, 32)   0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_52 (Conv2DTran (None, 128, 128, 16) 4624        activation_232[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 128, 128, 32) 0           conv2d_transpose_52[0][0]        \n",
      "                                                                 activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 128, 128, 32) 0           concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, 128, 128, 16) 4624        dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 128, 128, 16) 64          conv2d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 128, 128, 16) 0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)             (None, 128, 128, 16) 2320        activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 128, 128, 16) 64          conv2d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 128, 128, 16) 0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, 128, 128, 1)  17          activation_234[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,164,305\n",
      "Trainable params: 2,161,361\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 75s 3s/step - loss: 1.1302 - val_loss: 10.9891\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.6162 - val_loss: 2.4705\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.4035 - val_loss: 8.4594\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.3219 - val_loss: 1.4866\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 31s 1s/step - loss: 0.2693 - val_loss: 0.5956\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 64s 2s/step - loss: 0.2453 - val_loss: 0.5598\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 38s 1s/step - loss: 0.2255 - val_loss: 0.7112\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.2075 - val_loss: 0.9913\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 31s 1s/step - loss: 0.2001 - val_loss: 1.0233\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 40s 1s/step - loss: 0.1938 - val_loss: 0.9932\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.1844 - val_loss: 1.1876\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 43s 2s/step - loss: 0.1724 - val_loss: 0.8541\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.1584 - val_loss: 1.0851\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.1558 - val_loss: 1.0728\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.1539 - val_loss: 1.4794\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 31s 1s/step - loss: 0.1427 - val_loss: 0.8428\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.1320 - val_loss: 1.1365\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.1316 - val_loss: 1.1889\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.1286 - val_loss: 1.4169\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.1278 - val_loss: 0.9590\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.1248 - val_loss: 0.6884\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.1121 - val_loss: 0.7508\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.1069 - val_loss: 0.9482\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.1044 - val_loss: 1.2114\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.1064 - val_loss: 0.9420\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.1055 - val_loss: 1.0333\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0953 - val_loss: 1.1641\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.1097 - val_loss: 1.1034\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0941 - val_loss: 1.5728\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 39s 1s/step - loss: 0.0879 - val_loss: 1.0016\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0882 - val_loss: 1.0385\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0903 - val_loss: 1.1095\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0805 - val_loss: 0.8031\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0837 - val_loss: 0.7159\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0774 - val_loss: 0.8768\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0744 - val_loss: 0.7153\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0720 - val_loss: 1.1316\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0701 - val_loss: 0.9228\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0809 - val_loss: 1.1070\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.1103 - val_loss: 0.9836\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0896 - val_loss: 1.4293\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0795 - val_loss: 0.8216\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0761 - val_loss: 0.8457\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0720 - val_loss: 0.8646\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0702 - val_loss: 0.7615\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0645 - val_loss: 0.7285\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0708 - val_loss: 0.7479\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0636 - val_loss: 0.7155\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0619 - val_loss: 0.8409\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0647 - val_loss: 0.7507\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0603 - val_loss: 0.7309\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0577 - val_loss: 0.8159\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0581 - val_loss: 0.7449\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0691 - val_loss: 1.0846\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0779 - val_loss: 1.4965\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0711 - val_loss: 0.8769\n",
      "Epoch 57/100\n",
      "28/28 [==============================] - 43s 2s/step - loss: 0.0642 - val_loss: 0.9897\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 39s 1s/step - loss: 0.0659 - val_loss: 0.8007\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0610 - val_loss: 0.8201\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0601 - val_loss: 0.9052\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 38s 1s/step - loss: 0.0559 - val_loss: 0.8733\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0577 - val_loss: 0.7488\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0556 - val_loss: 0.8871\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0505 - val_loss: 0.7696\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0521 - val_loss: 0.8278\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0515 - val_loss: 0.7610\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0543 - val_loss: 0.8068\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0554 - val_loss: 0.8600\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0503 - val_loss: 0.8529\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0481 - val_loss: 0.6445\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0517 - val_loss: 1.0097\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 38s 1s/step - loss: 0.0512 - val_loss: 0.7670\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0478 - val_loss: 0.7640\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0480 - val_loss: 0.7684\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0525 - val_loss: 0.8358\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0511 - val_loss: 0.8106\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 31s 1s/step - loss: 0.0462 - val_loss: 0.8973\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0447 - val_loss: 0.7911\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0465 - val_loss: 0.8364\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0439 - val_loss: 0.8820\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0437 - val_loss: 0.8807\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0434 - val_loss: 0.7256\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0433 - val_loss: 0.8179\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0440 - val_loss: 0.6951\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0436 - val_loss: 0.7577\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 41s 1s/step - loss: 0.0442 - val_loss: 0.7899\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 41s 1s/step - loss: 0.0438 - val_loss: 0.7426\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 42s 1s/step - loss: 0.0418 - val_loss: 0.8477\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0399 - val_loss: 0.7027\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 55s 2s/step - loss: 0.0419 - val_loss: 0.8096\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 0.0414 - val_loss: 0.7683\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0403 - val_loss: 0.8164\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 38s 1s/step - loss: 0.0397 - val_loss: 0.8596\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0414 - val_loss: 0.7180\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 38s 1s/step - loss: 0.0428 - val_loss: 0.9389\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0431 - val_loss: 0.7781\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0385 - val_loss: 0.6921\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 31s 1s/step - loss: 0.0375 - val_loss: 0.7734\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0374 - val_loss: 0.8851\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0362 - val_loss: 0.7294\n"
     ]
    }
   ],
   "source": [
    "model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "#optimizer=keras.optimizers.Adadelta(lr=1000.0, rho=0.95)\n",
    "#sgd = keras.optimizers.SGD(lr=0.5, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# sgd = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "#print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(model.layers[0].get_weights()))\n",
    "sgd = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss=keras_custom_loss_function,\n",
    "          optimizer=sgd)\n",
    "#           metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit_generator(imageGen, steps_per_epoch=len(x_train), epochs=100, validation_data=(x_test, y_test)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size=20, epochs=12, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5d7f5c3b089e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "def show(x):\n",
    "    plt.figure()\n",
    "    plt.imshow((128*np.squeeze(x) + 128).astype(np.uint8), cmap='gray')\n",
    "\n",
    "for x, y in zip(x_train, y_train):\n",
    "    pred = model.predict(x.reshape(1,128,128,1))\n",
    "    show(x)\n",
    "    show(y)\n",
    "    show(pred)\n",
    "\n",
    "for x, y in zip(x_test, y_test):\n",
    "    pred = model.predict(x.reshape(1,128,128,1))\n",
    "    show(x)\n",
    "    show(y)\n",
    "    show(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
